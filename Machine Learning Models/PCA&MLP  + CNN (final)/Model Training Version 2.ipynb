{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbe6fd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Functions\n",
    "def trim_initial(df):\n",
    "    return df[START_DELAY:]\n",
    "\n",
    "def drop_zeroes(df):\n",
    "    return df.loc[df.ax != 0.0]\n",
    "\n",
    "def gen_rolling_window(df, pca=False):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    for uuid in df.uuid.unique():\n",
    "        processed_df = df.loc[df.uuid == uuid]\n",
    "        y_train_point = processed_df['exercise_type'].iloc[0]\n",
    "        processed_df = trim_initial(processed_df)\n",
    "        processed_df = drop_zeroes(processed_df)\n",
    "\n",
    "        # Loop through to extract rolling window\n",
    "        i = 0\n",
    "        while i + WINDOW < len(processed_df):\n",
    "            x_train_point = processed_df[i: i + WINDOW].filter(items=['ax', 'ay', 'az', 'gx', 'gy', 'gz'])\n",
    "            if (pca):\n",
    "                x_train_point = np.reshape(x_train_point.to_numpy(), -1)\n",
    "                x_train.append(x_train_point)\n",
    "            else:\n",
    "                x_train.append(x_train_point.to_numpy())\n",
    "            y_train.append(y_train_point)\n",
    "            i = i + ROLLING_STEP\n",
    "    x_train = np.array(x_train)\n",
    "    y_train = np.array(y_train)\n",
    "    y_train = to_categorical(y_train)\n",
    "    \n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c87bd136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Functions\n",
    "import scipy.linalg as linalg\n",
    "def ComputeNorm(x):\n",
    "    # function r=ComputeNorm(x)\n",
    "    # computes vector norms of x\n",
    "    # x: d x m matrix, each column a vector\n",
    "    # r: 1 x m matrix, each the corresponding norm (L2)\n",
    "\n",
    "    [row, col] = x.shape\n",
    "    r = np.zeros((1,col))\n",
    "\n",
    "    for i in range(col):\n",
    "        r[0,i] = linalg.norm(x[:,i])\n",
    "    return r\n",
    "\n",
    "def myLDA(A,Labels):\n",
    "    # function [W,m]=myLDA(A,Label)\n",
    "    # computes LDA of matrix A\n",
    "    # A: D by N data matrix. Each column is a random vector\n",
    "    # W: D by K matrix whose columns are the principal components in decreasing order\n",
    "    # m: mean of each projection\n",
    "    classLabels = np.unique(Labels)\n",
    "    classNum = len(classLabels)\n",
    "    dim,datanum = A.shape\n",
    "    totalMean = np.mean(A,1)\n",
    "    partition = [np.where(Labels==label)[0] for label in classLabels]\n",
    "    classMean = [(np.mean(A[:,idx],1),len(idx)) for idx in partition]\n",
    "\n",
    "    #compute the within-class scatter matrix\n",
    "    W = np.zeros((dim,dim))\n",
    "    for idx in partition:\n",
    "        W += np.cov(A[:,idx],rowvar=1)*len(idx)\n",
    "\n",
    "    #compute the between-class scatter matrix\n",
    "    B = np.zeros((dim,dim))\n",
    "    for mu,class_size in classMean:\n",
    "        offset = mu - totalMean\n",
    "        B += np.outer(offset,offset)*class_size\n",
    "\n",
    "    #solve the generalized eigenvalue problem for discriminant directions\n",
    "    ew, ev = linalg.eig(B, W)\n",
    "\n",
    "    sorted_pairs = sorted(enumerate(ew), key=operator.itemgetter(1), reverse=True)\n",
    "    selected_ind = [ind for ind,val in sorted_pairs[:classNum-1]]\n",
    "    LDAW = ev[:,selected_ind]\n",
    "    Centers = [np.dot(mu,LDAW) for mu,class_size in classMean]\n",
    "    Centers = np.array(Centers).T\n",
    "    return LDAW, Centers, classLabels\n",
    "\n",
    "def myPCA(A):\n",
    "    # function [W,LL,m]=mypca(A)\n",
    "    # computes PCA of matrix A\n",
    "    # A: D by N data matrix. Each column is a random vector\n",
    "    # W: D by K matrix whose columns are the principal components in decreasing order\n",
    "    # LL: eigenvalues\n",
    "    # m: mean of columns of A\n",
    "\n",
    "    # Note: \"lambda\" is a Python reserved word\n",
    "\n",
    "\n",
    "    # compute mean, and subtract mean from every column\n",
    "    [r,c] = A.shape\n",
    "    m = np.mean(A,1)\n",
    "    A = A - np.tile(m, (c,1)).T\n",
    "    B = np.dot(A.T, A)\n",
    "    [d,v] = linalg.eig(B)\n",
    "\n",
    "    # sort d in descending order\n",
    "    order_index = np.argsort(d)\n",
    "    order_index =  order_index[::-1]\n",
    "    d = d[order_index]\n",
    "    v = v[:, order_index]\n",
    "\n",
    "    # compute eigenvectors of scatter matrix\n",
    "    W = np.dot(A,v)\n",
    "    Wnorm = ComputeNorm(W)\n",
    "\n",
    "    W1 = np.tile(Wnorm, (r, 1))\n",
    "    W2 = W / W1\n",
    "    \n",
    "    LL = d[0:-1]\n",
    "\n",
    "    W = W2[:,0:-1]      #omit last column, which is the nullspace\n",
    "    \n",
    "    return W, LL, m\n",
    "\n",
    "def project_windows(windows, pcas, mean):\n",
    "    # Each col in faces represents a face\n",
    "    # Each col in pcas represents a pca\n",
    "    # Each col in pca_features represents a feature\n",
    "    # Transpose pca_features to before fitting into sci-kit model\n",
    "    [rows, cols] = windows.shape #cols is no. of faces\n",
    "    pca_features = pcas.T @ (windows - np.tile(mean, (cols, 1)).T)\n",
    "    return pca_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "012cfa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation Functions\n",
    "def evaluate_categorical_predictions(model, x_test, y_test):\n",
    "    reference = {0: \"Running\", 1: \"Push ups\", 2: \"Skipping\"}\n",
    "    right_predictions = {0:0, 1:0, 2:0}\n",
    "    totals =  {0:0, 1:0, 2:0}\n",
    "    predictions = model.predict(x_test)\n",
    "    for i in range(len(predictions)):\n",
    "        prediction = np.argmax(predictions[i])\n",
    "        actual = np.argmax(y_test[i])\n",
    "        print(\"Predicted: {} vs Actual: {}\".format(reference[prediction], reference[actual]))\n",
    "        totals[actual] += 1\n",
    "        if prediction == actual:\n",
    "            right_predictions[actual] += 1\n",
    "\n",
    "    print(\"\\n              ======================\")\n",
    "    print(\"Accuracy for Running: {.2f}%\", 100*right_predictions[0]/totals[0])\n",
    "    print(\"Accuracy for Push Ups: {.2f}%\", 100*right_predictions[1]/totals[1])\n",
    "    print(\"Accuracy for Skipping: {.2f}%\", 100*right_predictions[2]/totals[2])\n",
    "    \n",
    "def get_prediction(model, sample):\n",
    "    reference = {0: \"Running\", 1: \"Push ups\", 2: \"Skipping\"}\n",
    "    predictions = x.predict(sample)\n",
    "    return reference[np.argmax(predictions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "479d64b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Sequential, metrics\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow.keras.optimizers as optimizers\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe3d8574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "FILEPATH = \"./data.csv\"\n",
    "START_DELAY = 16 #rows to remove to account for ramp up time\n",
    "WINDOW = 33 # ~10s\n",
    "ROLLING_STEP = 3 # ~1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd2ae320",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensorID</th>\n",
       "      <th>uuid</th>\n",
       "      <th>timecollected</th>\n",
       "      <th>ax</th>\n",
       "      <th>ay</th>\n",
       "      <th>az</th>\n",
       "      <th>gx</th>\n",
       "      <th>gy</th>\n",
       "      <th>gz</th>\n",
       "      <th>exercise_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>c1f7b71a-3f7d-11ec-90f0-dcfb48770d48</td>\n",
       "      <td>2021-11-07T03:51:04.491502Z</td>\n",
       "      <td>0.978027</td>\n",
       "      <td>1.346191</td>\n",
       "      <td>1.154297</td>\n",
       "      <td>-2.914429</td>\n",
       "      <td>-3.166199</td>\n",
       "      <td>1.579285</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>c1f7b71a-3f7d-11ec-90f0-dcfb48770d48</td>\n",
       "      <td>2021-11-07T03:51:04.190568Z</td>\n",
       "      <td>0.978027</td>\n",
       "      <td>1.346191</td>\n",
       "      <td>1.154297</td>\n",
       "      <td>-2.914429</td>\n",
       "      <td>-3.166199</td>\n",
       "      <td>1.579285</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>c1f7b71a-3f7d-11ec-90f0-dcfb48770d48</td>\n",
       "      <td>2021-11-07T03:51:03.888082Z</td>\n",
       "      <td>0.978027</td>\n",
       "      <td>1.346191</td>\n",
       "      <td>1.154297</td>\n",
       "      <td>-2.914429</td>\n",
       "      <td>-3.166199</td>\n",
       "      <td>1.579285</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>c1f7b71a-3f7d-11ec-90f0-dcfb48770d48</td>\n",
       "      <td>2021-11-07T03:51:03.578131Z</td>\n",
       "      <td>1.641113</td>\n",
       "      <td>1.117188</td>\n",
       "      <td>0.715820</td>\n",
       "      <td>11.596680</td>\n",
       "      <td>73.837280</td>\n",
       "      <td>-8.659363</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>c1f7b71a-3f7d-11ec-90f0-dcfb48770d48</td>\n",
       "      <td>2021-11-07T03:51:03.276226Z</td>\n",
       "      <td>1.641113</td>\n",
       "      <td>1.117188</td>\n",
       "      <td>0.715820</td>\n",
       "      <td>11.596680</td>\n",
       "      <td>73.837280</td>\n",
       "      <td>-8.659363</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>2</td>\n",
       "      <td>c9494b52-3bc4-11ec-8ca8-dcfb48770d48</td>\n",
       "      <td>2021-11-02T10:08:14.229519Z</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>2</td>\n",
       "      <td>c9494b52-3bc4-11ec-8ca8-dcfb48770d48</td>\n",
       "      <td>2021-11-02T10:08:13.914417Z</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>2</td>\n",
       "      <td>c9494b52-3bc4-11ec-8ca8-dcfb48770d48</td>\n",
       "      <td>2021-11-02T10:08:13.602219Z</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>2</td>\n",
       "      <td>c9494b52-3bc4-11ec-8ca8-dcfb48770d48</td>\n",
       "      <td>2021-11-02T10:08:13.289409Z</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>2</td>\n",
       "      <td>c9494b52-3bc4-11ec-8ca8-dcfb48770d48</td>\n",
       "      <td>2021-11-02T10:08:13.030517Z</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>810 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sensorID                                  uuid  \\\n",
       "0           2  c1f7b71a-3f7d-11ec-90f0-dcfb48770d48   \n",
       "1           2  c1f7b71a-3f7d-11ec-90f0-dcfb48770d48   \n",
       "2           2  c1f7b71a-3f7d-11ec-90f0-dcfb48770d48   \n",
       "3           2  c1f7b71a-3f7d-11ec-90f0-dcfb48770d48   \n",
       "4           2  c1f7b71a-3f7d-11ec-90f0-dcfb48770d48   \n",
       "..        ...                                   ...   \n",
       "805         2  c9494b52-3bc4-11ec-8ca8-dcfb48770d48   \n",
       "806         2  c9494b52-3bc4-11ec-8ca8-dcfb48770d48   \n",
       "807         2  c9494b52-3bc4-11ec-8ca8-dcfb48770d48   \n",
       "808         2  c9494b52-3bc4-11ec-8ca8-dcfb48770d48   \n",
       "809         2  c9494b52-3bc4-11ec-8ca8-dcfb48770d48   \n",
       "\n",
       "                   timecollected        ax        ay        az         gx  \\\n",
       "0    2021-11-07T03:51:04.491502Z  0.978027  1.346191  1.154297  -2.914429   \n",
       "1    2021-11-07T03:51:04.190568Z  0.978027  1.346191  1.154297  -2.914429   \n",
       "2    2021-11-07T03:51:03.888082Z  0.978027  1.346191  1.154297  -2.914429   \n",
       "3    2021-11-07T03:51:03.578131Z  1.641113  1.117188  0.715820  11.596680   \n",
       "4    2021-11-07T03:51:03.276226Z  1.641113  1.117188  0.715820  11.596680   \n",
       "..                           ...       ...       ...       ...        ...   \n",
       "805  2021-11-02T10:08:14.229519Z  0.000000  0.000000  0.000000   0.000000   \n",
       "806  2021-11-02T10:08:13.914417Z  0.000000  0.000000  0.000000   0.000000   \n",
       "807  2021-11-02T10:08:13.602219Z  0.000000  0.000000  0.000000   0.000000   \n",
       "808  2021-11-02T10:08:13.289409Z  0.000000  0.000000  0.000000   0.000000   \n",
       "809  2021-11-02T10:08:13.030517Z  0.000000  0.000000  0.000000   0.000000   \n",
       "\n",
       "            gy        gz  exercise_type  \n",
       "0    -3.166199  1.579285              0  \n",
       "1    -3.166199  1.579285              0  \n",
       "2    -3.166199  1.579285              0  \n",
       "3    73.837280 -8.659363              0  \n",
       "4    73.837280 -8.659363              0  \n",
       "..         ...       ...            ...  \n",
       "805   0.000000  0.000000              1  \n",
       "806   0.000000  0.000000              1  \n",
       "807   0.000000  0.000000              1  \n",
       "808   0.000000  0.000000              1  \n",
       "809   0.000000  0.000000              1  \n",
       "\n",
       "[810 rows x 10 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading in the dataset\n",
    "df = pd.read_csv(FILEPATH)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d200d55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 22\n"
     ]
    }
   ],
   "source": [
    "# Processing the Dataset\n",
    "X, Y = gen_rolling_window(df)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, \n",
    "                                                    random_state = 1)\n",
    "print(len(x_train), len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f60c6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88, 33, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b655705",
   "metadata": {},
   "source": [
    "## First we will observe how LSTM and MLP perform "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99612e34",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d7e3d826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 2s 440ms/step - loss: 8.4958 - categorical_accuracy: 0.2614\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 1s 456ms/step - loss: 7.6478 - categorical_accuracy: 0.2727\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 1s 443ms/step - loss: 6.4453 - categorical_accuracy: 0.3295\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 1s 453ms/step - loss: 4.7863 - categorical_accuracy: 0.3864\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 1s 458ms/step - loss: 3.9894 - categorical_accuracy: 0.4432\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 1s 439ms/step - loss: 4.2036 - categorical_accuracy: 0.4091\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 1s 453ms/step - loss: 4.0608 - categorical_accuracy: 0.4659\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 1s 438ms/step - loss: 4.3336 - categorical_accuracy: 0.4773\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 1s 446ms/step - loss: 4.8443 - categorical_accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 1s 446ms/step - loss: 4.8801 - categorical_accuracy: 0.5000\n",
      "Calculating the Accuracy\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 4.7659 - categorical_accuracy: 0.3636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.765928268432617, 0.3636363744735718]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Using LSTM\n",
    "dropout = 0\n",
    "epochs = 10\n",
    "learning_rate = 0.00001\n",
    "FEATURES = 6\n",
    "\n",
    "lstm = Sequential()\n",
    "lstm.add(LSTM(1024, activation='relu', input_shape=(WINDOW, FEATURES), return_sequences=False))\n",
    "lstm.add(Dropout(dropout))\n",
    "lstm.add(Dense(3, activation='softmax'))\n",
    "# Create our optimizer\n",
    "optimizer = optimizers.Adam(learning_rate = learning_rate)\n",
    "lstm.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "lstm.fit(x_train, y_train, epochs=epochs, verbose=1)\n",
    "print('Calculating the Accuracy')\n",
    "lstm.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "455fecf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: Running vs Actual: Push ups\n",
      "Predicted: Push ups vs Actual: Running\n",
      "Predicted: Push ups vs Actual: Skipping\n",
      "Predicted: Running vs Actual: Push ups\n",
      "Predicted: Skipping vs Actual: Skipping\n",
      "Predicted: Skipping vs Actual: Push ups\n",
      "Predicted: Skipping vs Actual: Push ups\n",
      "Predicted: Skipping vs Actual: Skipping\n",
      "Predicted: Skipping vs Actual: Running\n",
      "Predicted: Running vs Actual: Running\n",
      "Predicted: Skipping vs Actual: Skipping\n",
      "Predicted: Running vs Actual: Push ups\n",
      "Predicted: Skipping vs Actual: Running\n",
      "Predicted: Running vs Actual: Push ups\n",
      "Predicted: Skipping vs Actual: Push ups\n",
      "Predicted: Push ups vs Actual: Running\n",
      "Predicted: Skipping vs Actual: Push ups\n",
      "\n",
      "              ======================\n",
      "Accuracy for Running: {.2f}% 20.0\n",
      "Accuracy for Push Ups: {.2f}% 0.0\n",
      "Accuracy for Skipping: {.2f}% 75.0\n"
     ]
    }
   ],
   "source": [
    "evaluate_categorical_predictions(lstm, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d54680",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "25ce86c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing the Dataset\n",
    "X, Y = gen_rolling_window(df)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.15, \n",
    "                                                    random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "eec3451b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93, 3)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "a9d2799f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    c:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\keras\\engine\\training.py:853 train_function  *\n        return step_function(self, iterator)\n    c:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\keras\\engine\\training.py:842 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    c:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    c:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    c:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    c:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\keras\\engine\\training.py:835 run_step  **\n        outputs = model.train_step(data)\n    c:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\keras\\engine\\training.py:789 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    c:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\keras\\engine\\compile_utils.py:201 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    c:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\keras\\losses.py:141 __call__\n        losses = call_fn(y_true, y_pred)\n    c:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\keras\\losses.py:245 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    c:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    c:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\keras\\losses.py:1666 categorical_crossentropy\n        y_true, y_pred, from_logits=from_logits, axis=axis)\n    c:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    c:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\keras\\backend.py:4839 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    c:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:1161 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (None, 3) and (None, 33, 3) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18180/4293905999.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"categorical_crossentropy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Calculating the Accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m#mlp.evaluate(x_test, y_test)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    758\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    759\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 760\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3064\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3065\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3066\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3067\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3463\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3306\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3307\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3308\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3309\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3310\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1007\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1008\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    666\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 668\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    669\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    992\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    993\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 994\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    995\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    c:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\keras\\engine\\training.py:853 train_function  *\n        return step_function(self, iterator)\n    c:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\keras\\engine\\training.py:842 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    c:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    c:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    c:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    c:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\keras\\engine\\training.py:835 run_step  **\n        outputs = model.train_step(data)\n    c:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\keras\\engine\\training.py:789 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    c:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\keras\\engine\\compile_utils.py:201 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    c:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\keras\\losses.py:141 __call__\n        losses = call_fn(y_true, y_pred)\n    c:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\keras\\losses.py:245 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    c:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    c:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\keras\\losses.py:1666 categorical_crossentropy\n        y_true, y_pred, from_logits=from_logits, axis=axis)\n    c:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    c:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\keras\\backend.py:4839 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    c:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:1161 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (None, 3) and (None, 33, 3) are incompatible\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0001\n",
    "momentum = 0.1\n",
    "decay = 0.02\n",
    "epochs = 25\n",
    "\n",
    "mlp = Sequential()\n",
    "mlp.add(Dense(1024, input_shape = (33, 6), activation = 'relu'))\n",
    "mlp.add(Dense(3, activation = 'softmax'))\n",
    "optimizer = optimizers.Adam(learning_rate=lr, decay=decay)\n",
    "\n",
    "mlp.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics='accuracy')\n",
    "\n",
    "mlp.fit(x_train, y_train, epochs=epochs, verbose=1)\n",
    "print('Calculating the Accuracy')\n",
    "#mlp.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea2f098",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_categorical_predictions(mlp, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5920785",
   "metadata": {},
   "source": [
    "## We will now attempt to train using Principal Component Analysis (PCA) and compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "271cbf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing the Dataset\n",
    "X, Y = gen_rolling_window(df, pca=True)\n",
    "pcas, eigenvalues, mean = myPCA(X.T)\n",
    "\n",
    "# retain only 30 eigenfaces/pcas\n",
    "D = 30\n",
    "pcas = pcas[:, :D]\n",
    "\n",
    "pca_features = project_windows(X.T, pcas, mean)\n",
    "x_train, x_test, y_train, y_test = train_test_split(pca_features.T, Y, test_size = 0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "36fc0416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pca file for future use\n",
    "\n",
    "import pickle\n",
    "with open(\"meanv2.txt\", \"wb\") as mean_file:\n",
    "    pickle.dump(mean, mean_file)\n",
    "    \n",
    "with open(\"pcasv2.txt\", \"wb\") as pcas_file:\n",
    "    pickle.dump(pcas, pcas_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "6a7fc3f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 110)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FEATURES = pca_features.shape\n",
    "FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "2841d247",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 11.1854 - accuracy: 0.4545\n",
      "Epoch 2/150\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 10.9880 - accuracy: 0.4545\n",
      "Epoch 3/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 10.7989 - accuracy: 0.4545\n",
      "Epoch 4/150\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 10.6551 - accuracy: 0.4545\n",
      "Epoch 5/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 10.5095 - accuracy: 0.4773\n",
      "Epoch 6/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 10.3887 - accuracy: 0.4773\n",
      "Epoch 7/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 10.2584 - accuracy: 0.4773\n",
      "Epoch 8/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 10.1337 - accuracy: 0.4773\n",
      "Epoch 9/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 10.0248 - accuracy: 0.5000\n",
      "Epoch 10/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 9.9259 - accuracy: 0.5000\n",
      "Epoch 11/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 9.8225 - accuracy: 0.4886\n",
      "Epoch 12/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 9.7201 - accuracy: 0.4886\n",
      "Epoch 13/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 9.6463 - accuracy: 0.4886\n",
      "Epoch 14/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 9.5460 - accuracy: 0.4886\n",
      "Epoch 15/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 9.4591 - accuracy: 0.4886\n",
      "Epoch 16/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 9.3805 - accuracy: 0.4886\n",
      "Epoch 17/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 9.3046 - accuracy: 0.4886\n",
      "Epoch 18/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 9.2300 - accuracy: 0.4886\n",
      "Epoch 19/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 9.1630 - accuracy: 0.5000\n",
      "Epoch 20/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 9.0892 - accuracy: 0.5000\n",
      "Epoch 21/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 9.0234 - accuracy: 0.5000\n",
      "Epoch 22/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 8.9480 - accuracy: 0.5000\n",
      "Epoch 23/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 8.8876 - accuracy: 0.5000\n",
      "Epoch 24/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 8.8314 - accuracy: 0.5000\n",
      "Epoch 25/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 8.7634 - accuracy: 0.5000\n",
      "Epoch 26/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 8.7114 - accuracy: 0.5000\n",
      "Epoch 27/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 8.6528 - accuracy: 0.5000\n",
      "Epoch 28/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 8.5991 - accuracy: 0.5000\n",
      "Epoch 29/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 8.5440 - accuracy: 0.5000\n",
      "Epoch 30/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 8.4898 - accuracy: 0.5000\n",
      "Epoch 31/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 8.4414 - accuracy: 0.5000\n",
      "Epoch 32/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 8.3881 - accuracy: 0.5000\n",
      "Epoch 33/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 8.3463 - accuracy: 0.5000\n",
      "Epoch 34/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 8.2976 - accuracy: 0.5000\n",
      "Epoch 35/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 8.2488 - accuracy: 0.5000\n",
      "Epoch 36/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 8.2066 - accuracy: 0.5000\n",
      "Epoch 37/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 8.1656 - accuracy: 0.5000\n",
      "Epoch 38/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 8.1203 - accuracy: 0.5000\n",
      "Epoch 39/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 8.0822 - accuracy: 0.5114\n",
      "Epoch 40/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 8.0421 - accuracy: 0.5114\n",
      "Epoch 41/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 8.0024 - accuracy: 0.5227\n",
      "Epoch 42/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 7.9624 - accuracy: 0.5227\n",
      "Epoch 43/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 7.9287 - accuracy: 0.5227\n",
      "Epoch 44/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 7.8913 - accuracy: 0.5227\n",
      "Epoch 45/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 7.8565 - accuracy: 0.5227\n",
      "Epoch 46/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 7.8228 - accuracy: 0.5227\n",
      "Epoch 47/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 7.7859 - accuracy: 0.5227\n",
      "Epoch 48/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 7.7560 - accuracy: 0.5227\n",
      "Epoch 49/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 7.7236 - accuracy: 0.5227\n",
      "Epoch 50/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 7.6924 - accuracy: 0.5227\n",
      "Epoch 51/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 7.6585 - accuracy: 0.5227\n",
      "Epoch 52/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 7.6318 - accuracy: 0.5227\n",
      "Epoch 53/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 7.5979 - accuracy: 0.5227\n",
      "Epoch 54/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 7.5717 - accuracy: 0.5114\n",
      "Epoch 55/150\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 7.5415 - accuracy: 0.5114\n",
      "Epoch 56/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 7.5144 - accuracy: 0.5114\n",
      "Epoch 57/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 7.4855 - accuracy: 0.5114\n",
      "Epoch 58/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 7.4596 - accuracy: 0.5227\n",
      "Epoch 59/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 7.4323 - accuracy: 0.5227\n",
      "Epoch 60/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 7.4031 - accuracy: 0.5227\n",
      "Epoch 61/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 7.3779 - accuracy: 0.5227\n",
      "Epoch 62/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 7.3569 - accuracy: 0.5227\n",
      "Epoch 63/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 7.3277 - accuracy: 0.5227\n",
      "Epoch 64/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 7.3054 - accuracy: 0.5227\n",
      "Epoch 65/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 7.2796 - accuracy: 0.5227\n",
      "Epoch 66/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 7.2556 - accuracy: 0.5227\n",
      "Epoch 67/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 7.2305 - accuracy: 0.5227\n",
      "Epoch 68/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 7.2080 - accuracy: 0.5227\n",
      "Epoch 69/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 7.1835 - accuracy: 0.5227\n",
      "Epoch 70/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 7.1608 - accuracy: 0.5227\n",
      "Epoch 71/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 7.1400 - accuracy: 0.5341\n",
      "Epoch 72/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 7.1164 - accuracy: 0.5341\n",
      "Epoch 73/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 7.0972 - accuracy: 0.5341\n",
      "Epoch 74/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 7.0730 - accuracy: 0.5341\n",
      "Epoch 75/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 7.0532 - accuracy: 0.5341\n",
      "Epoch 76/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 7.0315 - accuracy: 0.5341\n",
      "Epoch 77/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 7.0107 - accuracy: 0.5341\n",
      "Epoch 78/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.9899 - accuracy: 0.5341\n",
      "Epoch 79/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.9698 - accuracy: 0.5341\n",
      "Epoch 80/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.9522 - accuracy: 0.5341\n",
      "Epoch 81/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 6.9311 - accuracy: 0.5341\n",
      "Epoch 82/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.9098 - accuracy: 0.5341\n",
      "Epoch 83/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.8927 - accuracy: 0.5341\n",
      "Epoch 84/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.8734 - accuracy: 0.5341\n",
      "Epoch 85/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 6.8553 - accuracy: 0.5341\n",
      "Epoch 86/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.8369 - accuracy: 0.5341\n",
      "Epoch 87/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.8187 - accuracy: 0.5341\n",
      "Epoch 88/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.8010 - accuracy: 0.5341\n",
      "Epoch 89/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.7843 - accuracy: 0.5341\n",
      "Epoch 90/150\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 6.7652 - accuracy: 0.5341\n",
      "Epoch 91/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 6.7489 - accuracy: 0.5341\n",
      "Epoch 92/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.7323 - accuracy: 0.5341\n",
      "Epoch 93/150\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 6.7164 - accuracy: 0.5341\n",
      "Epoch 94/150\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 6.7006 - accuracy: 0.5341\n",
      "Epoch 95/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.6828 - accuracy: 0.5341\n",
      "Epoch 96/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.6669 - accuracy: 0.5341\n",
      "Epoch 97/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 6.6526 - accuracy: 0.5341\n",
      "Epoch 98/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 6.6370 - accuracy: 0.5341\n",
      "Epoch 99/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.6207 - accuracy: 0.5341\n",
      "Epoch 100/150\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 6.6072 - accuracy: 0.5341\n",
      "Epoch 101/150\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 6.5913 - accuracy: 0.5341\n",
      "Epoch 102/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.5764 - accuracy: 0.5341\n",
      "Epoch 103/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 6.5622 - accuracy: 0.5455\n",
      "Epoch 104/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.5479 - accuracy: 0.5455\n",
      "Epoch 105/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.5340 - accuracy: 0.5455\n",
      "Epoch 106/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 6.5189 - accuracy: 0.5455\n",
      "Epoch 107/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.5054 - accuracy: 0.5455\n",
      "Epoch 108/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.4927 - accuracy: 0.5455\n",
      "Epoch 109/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.4790 - accuracy: 0.5455\n",
      "Epoch 110/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.4651 - accuracy: 0.5455\n",
      "Epoch 111/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 6.4514 - accuracy: 0.5455\n",
      "Epoch 112/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.4400 - accuracy: 0.5455\n",
      "Epoch 113/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 6.4271 - accuracy: 0.5455\n",
      "Epoch 114/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.4136 - accuracy: 0.5455\n",
      "Epoch 115/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 6.4010 - accuracy: 0.5455\n",
      "Epoch 116/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.3889 - accuracy: 0.5455\n",
      "Epoch 117/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.3762 - accuracy: 0.5455\n",
      "Epoch 118/150\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 6.3644 - accuracy: 0.5455\n",
      "Epoch 119/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.3522 - accuracy: 0.5455\n",
      "Epoch 120/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 6.3415 - accuracy: 0.5455\n",
      "Epoch 121/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 6.3278 - accuracy: 0.5455\n",
      "Epoch 122/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.3186 - accuracy: 0.5455\n",
      "Epoch 123/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.3065 - accuracy: 0.5568\n",
      "Epoch 124/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 6.2952 - accuracy: 0.5568\n",
      "Epoch 125/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.2831 - accuracy: 0.5568\n",
      "Epoch 126/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.2724 - accuracy: 0.5568\n",
      "Epoch 127/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.2614 - accuracy: 0.5568\n",
      "Epoch 128/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.2509 - accuracy: 0.5568\n",
      "Epoch 129/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 6.2408 - accuracy: 0.5568\n",
      "Epoch 130/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.2305 - accuracy: 0.5568\n",
      "Epoch 131/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 6.2200 - accuracy: 0.5568\n",
      "Epoch 132/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 6.2079 - accuracy: 0.5568\n",
      "Epoch 133/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 6.1983 - accuracy: 0.5568\n",
      "Epoch 134/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 6.1879 - accuracy: 0.5568\n",
      "Epoch 135/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.1789 - accuracy: 0.5682\n",
      "Epoch 136/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 6.1675 - accuracy: 0.5682\n",
      "Epoch 137/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.1580 - accuracy: 0.5682\n",
      "Epoch 138/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 6.1470 - accuracy: 0.5682\n",
      "Epoch 139/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 6.1370 - accuracy: 0.5682\n",
      "Epoch 140/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 6.1279 - accuracy: 0.5682\n",
      "Epoch 141/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.1174 - accuracy: 0.5682\n",
      "Epoch 142/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.1076 - accuracy: 0.5682\n",
      "Epoch 143/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.0983 - accuracy: 0.5682\n",
      "Epoch 144/150\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 6.0884 - accuracy: 0.5682\n",
      "Epoch 145/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.0786 - accuracy: 0.5682\n",
      "Epoch 146/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.0693 - accuracy: 0.5795\n",
      "Epoch 147/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.0593 - accuracy: 0.5795\n",
      "Epoch 148/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.0511 - accuracy: 0.5795\n",
      "Epoch 149/150\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 6.0407 - accuracy: 0.5795\n",
      "Epoch 150/150\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 6.0315 - accuracy: 0.5795\n",
      "Calculating the Accuracy\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 14.4534 - accuracy: 0.2273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[14.453357696533203, 0.22727273404598236]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FEATURES = pca_features.shape\n",
    "learning_rate = 0.000001\n",
    "momentum = 0.01\n",
    "decay = 0.02\n",
    "epochs = 150\n",
    "\n",
    "pca_mlp = Sequential(name=\"PCA_MLP\")\n",
    "pca_mlp.add(Dense(1024, input_shape = (30, ), activation = 'relu'))\n",
    "pca_mlp.add(Dense(512, activation = 'relu'))\n",
    "pca_mlp.add(Dense(512, activation = 'relu'))\n",
    "pca_mlp.add(Dense(3, activation = 'softmax'))\n",
    "optimizer = optimizers.Adam(learning_rate=learning_rate, decay=decay)\n",
    "\n",
    "pca_mlp.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics='accuracy')\n",
    "\n",
    "pca_mlp.fit(x_train, y_train, epochs=epochs, verbose=1)\n",
    "print('Calculating the Accuracy')\n",
    "pca_mlp.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "9436acce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: modelv2.b\\assets\n"
     ]
    }
   ],
   "source": [
    "pca_mlp.save('modelv2.b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed63557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "60dca998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: Push ups vs Actual: Push ups\n",
      "Predicted: Running vs Actual: Running\n",
      "Predicted: Running vs Actual: Skipping\n",
      "Predicted: Push ups vs Actual: Push ups\n",
      "Predicted: Skipping vs Actual: Skipping\n",
      "Predicted: Push ups vs Actual: Push ups\n",
      "Predicted: Push ups vs Actual: Push ups\n",
      "Predicted: Skipping vs Actual: Skipping\n",
      "Predicted: Running vs Actual: Running\n",
      "Predicted: Running vs Actual: Running\n",
      "Predicted: Skipping vs Actual: Skipping\n",
      "\n",
      "              ======================\n",
      "Accuracy for Running: {.2f}% 100.0\n",
      "Accuracy for Push Ups: {.2f}% 100.0\n",
      "Accuracy for Skipping: {.2f}% 75.0\n",
      "\n",
      "          =================== \n",
      "\n",
      "Running dataset: {} 54\n",
      "Push-Ups dataset: {} 30\n",
      "Skipping dataset: {} 26\n"
     ]
    }
   ],
   "source": [
    "evaluate_categorical_predictions(pca_mlp, x_test, y_test)\n",
    "\n",
    "print(\"\\n          =================== \\n\")\n",
    "count = [0, 0, 0]\n",
    "for y in Y:\n",
    "    count[np.argmax(y)] += 1\n",
    "\n",
    "print(\"Running dataset: {}\", count[0])\n",
    "print(\"Push-Ups dataset: {}\", count[1])\n",
    "print(\"Skipping dataset: {}\", count[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76282bfe",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf5a7136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing the Dataset\n",
    "X, Y = gen_rolling_window(df)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.15, \n",
    "                                                    random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b189050c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93, 33, 6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b1bd163",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.99707031e-01,  2.08544922e+00,  2.25097656e-01,\n",
       "         1.11480713e+02,  1.40991211e+01, -4.88204956e+01],\n",
       "       [ 1.99707031e-01,  2.08544922e+00,  2.25097656e-01,\n",
       "         1.11480713e+02,  1.40991211e+01, -4.88204956e+01],\n",
       "       [ 1.99707031e-01,  2.08544922e+00,  2.25097656e-01,\n",
       "         1.11480713e+02,  1.40991211e+01, -4.88204956e+01],\n",
       "       [-4.15527344e-01,  1.76367188e+00,  9.38476562e-01,\n",
       "         4.87136841e+01,  1.42288208e+01, -3.34930420e+01],\n",
       "       [-4.15527344e-01,  1.76367188e+00,  9.38476562e-01,\n",
       "         4.87136841e+01,  1.42288208e+01, -3.34930420e+01],\n",
       "       [-4.15527344e-01,  1.76367188e+00,  9.38476562e-01,\n",
       "         4.87136841e+01,  1.42288208e+01, -3.34930420e+01],\n",
       "       [-2.26562500e-01,  1.99951172e+00,  2.11425781e-01,\n",
       "        -1.05972290e+01,  2.93731689e+00,  1.31988525e+00],\n",
       "       [-2.26562500e-01,  1.99951172e+00,  2.11425781e-01,\n",
       "        -1.05972290e+01,  2.93731689e+00,  1.31988525e+00],\n",
       "       [-2.26562500e-01,  1.99951172e+00,  2.11425781e-01,\n",
       "        -1.05972290e+01,  2.93731689e+00,  1.31988525e+00],\n",
       "       [ 2.14843750e-01,  1.86328125e+00,  7.83203125e-01,\n",
       "         1.32751465e+00, -6.41632080e+00,  5.71441650e+00],\n",
       "       [ 2.14843750e-01,  1.86328125e+00,  7.83203125e-01,\n",
       "         1.32751465e+00, -6.41632080e+00,  5.71441650e+00],\n",
       "       [ 2.14843750e-01,  1.86328125e+00,  7.83203125e-01,\n",
       "         1.32751465e+00, -6.41632080e+00,  5.71441650e+00],\n",
       "       [ 5.27343750e-02,  2.07421875e+00,  1.91406250e-01,\n",
       "        -1.39846802e+01, -9.38415527e-01,  8.81958008e+00],\n",
       "       [ 5.27343750e-02,  2.07421875e+00,  1.91406250e-01,\n",
       "        -1.39846802e+01, -9.38415527e-01,  8.81958008e+00],\n",
       "       [ 5.27343750e-02,  2.07421875e+00,  1.91406250e-01,\n",
       "        -1.39846802e+01, -9.38415527e-01,  8.81958008e+00],\n",
       "       [ 5.27343750e-02,  2.07421875e+00,  1.91406250e-01,\n",
       "        -1.39846802e+01, -9.38415527e-01,  8.81958008e+00],\n",
       "       [-4.40429688e-01,  2.01318359e+00,  6.36718750e-01,\n",
       "         7.70797729e+01,  6.58416748e+00, -1.20849609e+01],\n",
       "       [-4.40429688e-01,  2.01318359e+00,  6.36718750e-01,\n",
       "         7.70797729e+01,  6.58416748e+00, -1.20849609e+01],\n",
       "       [-4.40429688e-01,  2.01318359e+00,  6.36718750e-01,\n",
       "         7.70797729e+01,  6.58416748e+00, -1.20849609e+01],\n",
       "       [-2.01660156e-01,  1.87890625e+00,  8.43261719e-01,\n",
       "         2.51770020e+01, -7.09533691e-01, -1.37176514e+01],\n",
       "       [-2.01660156e-01,  1.87890625e+00,  8.43261719e-01,\n",
       "         2.51770020e+01, -7.09533691e-01, -1.37176514e+01],\n",
       "       [-2.01660156e-01,  1.87890625e+00,  8.43261719e-01,\n",
       "         2.51770020e+01, -7.09533691e-01, -1.37176514e+01],\n",
       "       [ 1.10839844e-01,  2.05126953e+00,  2.08007812e-01,\n",
       "         2.28118896e+00,  3.79943848e+00,  7.86590576e+00],\n",
       "       [ 1.10839844e-01,  2.05126953e+00,  2.08007812e-01,\n",
       "         2.28118896e+00,  3.79943848e+00,  7.86590576e+00],\n",
       "       [ 1.10839844e-01,  2.05126953e+00,  2.08007812e-01,\n",
       "         2.28118896e+00,  3.79943848e+00,  7.86590576e+00],\n",
       "       [-2.69042969e-01,  1.79980469e+00,  9.73144531e-01,\n",
       "         1.79290771e+01,  5.16510010e+00,  7.54547119e+00],\n",
       "       [-2.69042969e-01,  1.79980469e+00,  9.73144531e-01,\n",
       "         1.79290771e+01,  5.16510010e+00,  7.54547119e+00],\n",
       "       [-2.69042969e-01,  1.79980469e+00,  9.73144531e-01,\n",
       "         1.79290771e+01,  5.16510010e+00,  7.54547119e+00],\n",
       "       [ 4.24316406e-01,  1.76855469e+00,  3.25195312e-01,\n",
       "        -4.37469482e+01,  1.69677734e+01,  7.49969482e+00],\n",
       "       [ 4.24316406e-01,  1.76855469e+00,  3.25195312e-01,\n",
       "        -4.37469482e+01,  1.69677734e+01,  7.49969482e+00],\n",
       "       [ 4.24316406e-01,  1.76855469e+00,  3.25195312e-01,\n",
       "        -4.37469482e+01,  1.69677734e+01,  7.49969482e+00],\n",
       "       [-5.37109375e-02,  1.87646484e+00,  7.46582031e-01,\n",
       "         7.98339844e+01,  2.32467651e+01, -8.69750977e+00],\n",
       "       [-5.37109375e-02,  1.87646484e+00,  7.46582031e-01,\n",
       "         7.98339844e+01,  2.32467651e+01, -8.69750977e+00]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd64baa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "def make_model():\n",
    "    input_layer = tf.keras.layers.Input((33,6))\n",
    "\n",
    "    conv1 = tf.keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(input_layer)\n",
    "    conv1 = tf.keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = tf.keras.layers.ReLU()(conv1)\n",
    "\n",
    "    conv2 = tf.keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv1)\n",
    "    conv2 = tf.keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = tf.keras.layers.ReLU()(conv2)\n",
    "\n",
    "    conv3 = tf.keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv2)\n",
    "    conv3 = tf.keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = tf.keras.layers.ReLU()(conv3)\n",
    "    \n",
    "    dense1 = Dense(1024, activation = 'relu')(conv3)\n",
    "\n",
    "    gap = tf.keras.layers.GlobalAveragePooling1D()(dense1)\n",
    "\n",
    "    output_layer = tf.keras.layers.Dense(3, activation=\"softmax\")(gap)\n",
    "\n",
    "    return tf.keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "\n",
    "model = make_model()\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6cb4394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "2/2 [==============================] - 1s 213ms/step - loss: 1.1234 - categorical_accuracy: 0.1351 - val_loss: 0.7728 - val_categorical_accuracy: 0.3684\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.8051 - categorical_accuracy: 0.7027 - val_loss: 1.2753 - val_categorical_accuracy: 0.3158\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6715 - categorical_accuracy: 0.6892 - val_loss: 1.5323 - val_categorical_accuracy: 0.3158\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4897 - categorical_accuracy: 0.9459 - val_loss: 1.2033 - val_categorical_accuracy: 0.3158\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.3999 - categorical_accuracy: 0.9324 - val_loss: 1.1283 - val_categorical_accuracy: 0.3158\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3206 - categorical_accuracy: 0.9459 - val_loss: 1.1746 - val_categorical_accuracy: 0.3158\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2440 - categorical_accuracy: 0.9324 - val_loss: 1.4565 - val_categorical_accuracy: 0.3158\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2162 - categorical_accuracy: 0.9595 - val_loss: 1.5372 - val_categorical_accuracy: 0.3158\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1592 - categorical_accuracy: 0.9730 - val_loss: 1.3400 - val_categorical_accuracy: 0.3158\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1440 - categorical_accuracy: 0.9730 - val_loss: 1.4389 - val_categorical_accuracy: 0.3158\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1167 - categorical_accuracy: 0.9730 - val_loss: 1.9108 - val_categorical_accuracy: 0.3158\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1412 - categorical_accuracy: 0.9730 - val_loss: 2.2484 - val_categorical_accuracy: 0.3158\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0841 - categorical_accuracy: 0.9730 - val_loss: 2.4442 - val_categorical_accuracy: 0.3158\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0714 - categorical_accuracy: 1.0000 - val_loss: 2.7712 - val_categorical_accuracy: 0.3158\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0609 - categorical_accuracy: 1.0000 - val_loss: 2.9827 - val_categorical_accuracy: 0.3158\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0735 - categorical_accuracy: 0.9865 - val_loss: 3.3412 - val_categorical_accuracy: 0.3158\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0564 - categorical_accuracy: 1.0000 - val_loss: 3.6709 - val_categorical_accuracy: 0.3158\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0335 - categorical_accuracy: 1.0000 - val_loss: 3.8543 - val_categorical_accuracy: 0.3158\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0295 - categorical_accuracy: 1.0000 - val_loss: 3.9651 - val_categorical_accuracy: 0.3684\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0248 - categorical_accuracy: 1.0000 - val_loss: 4.0009 - val_categorical_accuracy: 0.3684\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0378 - categorical_accuracy: 1.0000 - val_loss: 3.9808 - val_categorical_accuracy: 0.3684\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0715 - categorical_accuracy: 0.9730 - val_loss: 3.4856 - val_categorical_accuracy: 0.3684\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0246 - categorical_accuracy: 1.0000 - val_loss: 2.6168 - val_categorical_accuracy: 0.3684\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0209 - categorical_accuracy: 1.0000 - val_loss: 1.8619 - val_categorical_accuracy: 0.4211\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0169 - categorical_accuracy: 1.0000 - val_loss: 1.2862 - val_categorical_accuracy: 0.4737\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0140 - categorical_accuracy: 1.0000 - val_loss: 0.8521 - val_categorical_accuracy: 0.4737\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0133 - categorical_accuracy: 1.0000 - val_loss: 0.5732 - val_categorical_accuracy: 0.6842\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0128 - categorical_accuracy: 1.0000 - val_loss: 0.3941 - val_categorical_accuracy: 0.8947\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0128 - categorical_accuracy: 1.0000 - val_loss: 0.2937 - val_categorical_accuracy: 0.8947\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0132 - categorical_accuracy: 1.0000 - val_loss: 0.2393 - val_categorical_accuracy: 0.9474\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0106 - categorical_accuracy: 1.0000 - val_loss: 0.2042 - val_categorical_accuracy: 0.9474\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0092 - categorical_accuracy: 1.0000 - val_loss: 0.1810 - val_categorical_accuracy: 0.9474\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0082 - categorical_accuracy: 1.0000 - val_loss: 0.1624 - val_categorical_accuracy: 0.9474\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0086 - categorical_accuracy: 1.0000 - val_loss: 0.1486 - val_categorical_accuracy: 0.9474\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0075 - categorical_accuracy: 1.0000 - val_loss: 0.1352 - val_categorical_accuracy: 0.9474\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0153 - categorical_accuracy: 1.0000 - val_loss: 0.1230 - val_categorical_accuracy: 0.9474\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0065 - categorical_accuracy: 1.0000 - val_loss: 0.1157 - val_categorical_accuracy: 0.9474\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0075 - categorical_accuracy: 1.0000 - val_loss: 0.1103 - val_categorical_accuracy: 0.9474\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0062 - categorical_accuracy: 1.0000 - val_loss: 0.1033 - val_categorical_accuracy: 0.9474\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0125 - categorical_accuracy: 1.0000 - val_loss: 0.0993 - val_categorical_accuracy: 0.9474\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0049 - categorical_accuracy: 1.0000 - val_loss: 0.0984 - val_categorical_accuracy: 0.9474\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0047 - categorical_accuracy: 1.0000 - val_loss: 0.0956 - val_categorical_accuracy: 0.9474\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0050 - categorical_accuracy: 1.0000 - val_loss: 0.0934 - val_categorical_accuracy: 0.9474\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0077 - categorical_accuracy: 1.0000 - val_loss: 0.0913 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0050 - categorical_accuracy: 1.0000 - val_loss: 0.0860 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0042 - categorical_accuracy: 1.0000 - val_loss: 0.0801 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0039 - categorical_accuracy: 1.0000 - val_loss: 0.0744 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0053 - categorical_accuracy: 1.0000 - val_loss: 0.0683 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0040 - categorical_accuracy: 1.0000 - val_loss: 0.0629 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0036 - categorical_accuracy: 1.0000 - val_loss: 0.0561 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0175 - categorical_accuracy: 1.0000 - val_loss: 0.0490 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0031 - categorical_accuracy: 1.0000 - val_loss: 0.0410 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0030 - categorical_accuracy: 1.0000 - val_loss: 0.0359 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0029 - categorical_accuracy: 1.0000 - val_loss: 0.0322 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0032 - categorical_accuracy: 1.0000 - val_loss: 0.0296 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0046 - categorical_accuracy: 1.0000 - val_loss: 0.0276 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0067 - categorical_accuracy: 1.0000 - val_loss: 0.0260 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0041 - categorical_accuracy: 1.0000 - val_loss: 0.0243 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0031 - categorical_accuracy: 1.0000 - val_loss: 0.0230 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0052 - categorical_accuracy: 1.0000 - val_loss: 0.0219 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0053 - categorical_accuracy: 1.0000 - val_loss: 0.0208 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0057 - categorical_accuracy: 1.0000 - val_loss: 0.0196 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0032 - categorical_accuracy: 1.0000 - val_loss: 0.0183 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0032 - categorical_accuracy: 1.0000 - val_loss: 0.0170 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.0124 - categorical_accuracy: 1.0000 - val_loss: 0.0153 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0024 - categorical_accuracy: 1.0000 - val_loss: 0.0152 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0021 - categorical_accuracy: 1.0000 - val_loss: 0.0166 - val_categorical_accuracy: 1.0000\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0023 - categorical_accuracy: 1.0000 - val_loss: 0.0187 - val_categorical_accuracy: 1.0000\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0022 - categorical_accuracy: 1.0000 - val_loss: 0.0214 - val_categorical_accuracy: 1.0000\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0020 - categorical_accuracy: 1.0000 - val_loss: 0.0237 - val_categorical_accuracy: 1.0000\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0019 - categorical_accuracy: 1.0000 - val_loss: 0.0255 - val_categorical_accuracy: 1.0000\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0015 - categorical_accuracy: 1.0000 - val_loss: 0.0262 - val_categorical_accuracy: 1.0000\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0017 - categorical_accuracy: 1.0000 - val_loss: 0.0268 - val_categorical_accuracy: 1.0000\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0019 - categorical_accuracy: 1.0000 - val_loss: 0.0268 - val_categorical_accuracy: 1.0000\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0017 - categorical_accuracy: 1.0000 - val_loss: 0.0264 - val_categorical_accuracy: 1.0000\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0033 - categorical_accuracy: 1.0000 - val_loss: 0.0242 - val_categorical_accuracy: 1.0000\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0015 - categorical_accuracy: 1.0000 - val_loss: 0.0216 - val_categorical_accuracy: 1.0000\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0028 - categorical_accuracy: 1.0000 - val_loss: 0.0189 - val_categorical_accuracy: 1.0000\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0013 - categorical_accuracy: 1.0000 - val_loss: 0.0169 - val_categorical_accuracy: 1.0000\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0065 - categorical_accuracy: 1.0000 - val_loss: 0.0156 - val_categorical_accuracy: 1.0000\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0055 - categorical_accuracy: 1.0000 - val_loss: 0.0170 - val_categorical_accuracy: 1.0000\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0015 - categorical_accuracy: 1.0000 - val_loss: 0.0202 - val_categorical_accuracy: 1.0000\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0017 - categorical_accuracy: 1.0000 - val_loss: 0.0237 - val_categorical_accuracy: 1.0000\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0014 - categorical_accuracy: 1.0000 - val_loss: 0.0263 - val_categorical_accuracy: 1.0000\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0018 - categorical_accuracy: 1.0000 - val_loss: 0.0279 - val_categorical_accuracy: 1.0000\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0085 - categorical_accuracy: 1.0000 - val_loss: 0.0266 - val_categorical_accuracy: 1.0000\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0023 - categorical_accuracy: 1.0000 - val_loss: 0.0234 - val_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0028 - categorical_accuracy: 1.0000 - val_loss: 0.0203 - val_categorical_accuracy: 1.0000\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0028 - categorical_accuracy: 1.0000 - val_loss: 0.0174 - val_categorical_accuracy: 1.0000\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0038 - categorical_accuracy: 1.0000 - val_loss: 0.0144 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0025 - categorical_accuracy: 1.0000 - val_loss: 0.0124 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0053 - categorical_accuracy: 1.0000 - val_loss: 0.0111 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0127 - categorical_accuracy: 1.0000 - val_loss: 0.0105 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0017 - categorical_accuracy: 1.0000 - val_loss: 0.0113 - val_categorical_accuracy: 1.0000\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0089 - categorical_accuracy: 1.0000 - val_loss: 0.0102 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0014 - categorical_accuracy: 1.0000 - val_loss: 0.0079 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0019 - categorical_accuracy: 1.0000 - val_loss: 0.0064 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0016 - categorical_accuracy: 1.0000 - val_loss: 0.0053 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0017 - categorical_accuracy: 1.0000 - val_loss: 0.0046 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0018 - categorical_accuracy: 1.0000 - val_loss: 0.0040 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0034 - categorical_accuracy: 1.0000 - val_loss: 0.0037 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0016 - categorical_accuracy: 1.0000 - val_loss: 0.0034 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0013 - categorical_accuracy: 1.0000 - val_loss: 0.0031 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0013 - categorical_accuracy: 1.0000 - val_loss: 0.0029 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0027 - categorical_accuracy: 1.0000 - val_loss: 0.0027 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0012 - categorical_accuracy: 1.0000 - val_loss: 0.0025 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0043 - categorical_accuracy: 1.0000 - val_loss: 0.0022 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0027 - categorical_accuracy: 1.0000 - val_loss: 0.0019 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0029 - categorical_accuracy: 1.0000 - val_loss: 0.0016 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0012 - categorical_accuracy: 1.0000 - val_loss: 0.0014 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 8.6724e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0013 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 8.5136e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0012 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 8.4639e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0011 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 9.5005e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0011 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0023 - categorical_accuracy: 1.0000 - val_loss: 0.0010 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0018 - categorical_accuracy: 1.0000 - val_loss: 9.6946e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0013 - categorical_accuracy: 1.0000 - val_loss: 9.1673e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 118/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0020 - categorical_accuracy: 1.0000 - val_loss: 8.7208e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 119/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0314 - categorical_accuracy: 0.9865 - val_loss: 8.6620e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 120/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0023 - categorical_accuracy: 1.0000 - val_loss: 9.4779e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 121/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 7.6158e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0011 - val_categorical_accuracy: 1.0000\n",
      "Epoch 122/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 8.9226e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0013 - val_categorical_accuracy: 1.0000\n",
      "Epoch 123/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 9.7952e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0015 - val_categorical_accuracy: 1.0000\n",
      "Epoch 124/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0013 - categorical_accuracy: 1.0000 - val_loss: 0.0017 - val_categorical_accuracy: 1.0000\n",
      "Epoch 125/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0011 - categorical_accuracy: 1.0000 - val_loss: 0.0019 - val_categorical_accuracy: 1.0000\n",
      "Epoch 126/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0020 - categorical_accuracy: 1.0000 - val_loss: 0.0020 - val_categorical_accuracy: 1.0000\n",
      "Epoch 127/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0020 - categorical_accuracy: 1.0000 - val_loss: 0.0021 - val_categorical_accuracy: 1.0000\n",
      "Epoch 128/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 48ms/step - loss: 0.0010 - categorical_accuracy: 1.0000 - val_loss: 0.0021 - val_categorical_accuracy: 1.0000\n",
      "Epoch 129/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0040 - categorical_accuracy: 1.0000 - val_loss: 0.0020 - val_categorical_accuracy: 1.0000\n",
      "Epoch 130/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0014 - categorical_accuracy: 1.0000 - val_loss: 0.0018 - val_categorical_accuracy: 1.0000\n",
      "Epoch 131/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0010 - categorical_accuracy: 1.0000 - val_loss: 0.0017 - val_categorical_accuracy: 1.0000\n",
      "Epoch 132/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0012 - categorical_accuracy: 1.0000 - val_loss: 0.0016 - val_categorical_accuracy: 1.0000\n",
      "Epoch 133/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0045 - categorical_accuracy: 1.0000 - val_loss: 0.0014 - val_categorical_accuracy: 1.0000\n",
      "Epoch 134/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0037 - categorical_accuracy: 1.0000 - val_loss: 0.0012 - val_categorical_accuracy: 1.0000\n",
      "Epoch 135/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0015 - categorical_accuracy: 1.0000 - val_loss: 0.0011 - val_categorical_accuracy: 1.0000\n",
      "Epoch 136/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0021 - categorical_accuracy: 1.0000 - val_loss: 9.7184e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 137/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0011 - categorical_accuracy: 1.0000 - val_loss: 9.3718e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 138/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0020 - categorical_accuracy: 1.0000 - val_loss: 9.2504e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 139/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 8.4693e-04 - categorical_accuracy: 1.0000 - val_loss: 9.0892e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 140/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0047 - categorical_accuracy: 1.0000 - val_loss: 8.8554e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 141/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 9.7714e-04 - categorical_accuracy: 1.0000 - val_loss: 8.6288e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 142/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.0061 - categorical_accuracy: 1.0000 - val_loss: 8.2666e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 143/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 7.0785e-04 - categorical_accuracy: 1.0000 - val_loss: 7.7652e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 144/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 8.5986e-04 - categorical_accuracy: 1.0000 - val_loss: 7.4140e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 145/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 6.8075e-04 - categorical_accuracy: 1.0000 - val_loss: 7.1464e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 146/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0010 - categorical_accuracy: 1.0000 - val_loss: 6.9519e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 147/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 7.5274e-04 - categorical_accuracy: 1.0000 - val_loss: 6.8029e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 148/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 6.3107e-04 - categorical_accuracy: 1.0000 - val_loss: 6.6389e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 149/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 8.5598e-04 - categorical_accuracy: 1.0000 - val_loss: 6.4656e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 150/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 7.2481e-04 - categorical_accuracy: 1.0000 - val_loss: 6.3261e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 151/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 6.6402e-04 - categorical_accuracy: 1.0000 - val_loss: 6.2355e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 152/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 6.8772e-04 - categorical_accuracy: 1.0000 - val_loss: 6.2028e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 153/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0018 - categorical_accuracy: 1.0000 - val_loss: 6.1743e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 154/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 8.5285e-04 - categorical_accuracy: 1.0000 - val_loss: 6.0773e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 155/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 7.9663e-04 - categorical_accuracy: 1.0000 - val_loss: 6.0129e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 156/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0017 - categorical_accuracy: 1.0000 - val_loss: 5.9125e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 157/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0016 - categorical_accuracy: 1.0000 - val_loss: 5.7680e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 158/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 5.6232e-04 - categorical_accuracy: 1.0000 - val_loss: 5.6898e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 159/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 7.2856e-04 - categorical_accuracy: 1.0000 - val_loss: 5.5765e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 160/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 6.4803e-04 - categorical_accuracy: 1.0000 - val_loss: 5.5146e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 161/300\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 9.7962e-04 - categorical_accuracy: 1.0000 - val_loss: 5.4092e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 162/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 5.4776e-04 - categorical_accuracy: 1.0000 - val_loss: 5.3432e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 163/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 6.9003e-04 - categorical_accuracy: 1.0000 - val_loss: 5.3018e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 164/300\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 5.4262e-04 - categorical_accuracy: 1.0000 - val_loss: 5.2750e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 165/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 5.3391e-04 - categorical_accuracy: 1.0000 - val_loss: 5.2353e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 166/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 5.3827e-04 - categorical_accuracy: 1.0000 - val_loss: 5.1756e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 167/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0015 - categorical_accuracy: 1.0000 - val_loss: 5.1026e-04 - val_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 168/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0019 - categorical_accuracy: 1.0000 - val_loss: 5.0398e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 169/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 4.2819e-04 - categorical_accuracy: 1.0000 - val_loss: 4.9535e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 170/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 6.9243e-04 - categorical_accuracy: 1.0000 - val_loss: 4.8881e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 171/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 7.0851e-04 - categorical_accuracy: 1.0000 - val_loss: 4.8568e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 172/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 5.1215e-04 - categorical_accuracy: 1.0000 - val_loss: 4.8460e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 173/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0013 - categorical_accuracy: 1.0000 - val_loss: 4.7713e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 174/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0191 - categorical_accuracy: 1.0000 - val_loss: 4.8058e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 175/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 4.6439e-04 - categorical_accuracy: 1.0000 - val_loss: 4.9562e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 176/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 8.2867e-04 - categorical_accuracy: 1.0000 - val_loss: 5.1652e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 177/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 4.5753e-04 - categorical_accuracy: 1.0000 - val_loss: 5.3514e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 178/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 7.4106e-04 - categorical_accuracy: 1.0000 - val_loss: 5.5220e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 179/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 7.2326e-04 - categorical_accuracy: 1.0000 - val_loss: 5.6550e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 180/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 5.0186e-04 - categorical_accuracy: 1.0000 - val_loss: 5.7694e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 181/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 9.3226e-04 - categorical_accuracy: 1.0000 - val_loss: 5.8314e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 182/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 8.2801e-04 - categorical_accuracy: 1.0000 - val_loss: 5.8996e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 183/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 7.7519e-04 - categorical_accuracy: 1.0000 - val_loss: 5.9238e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 184/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 8.5345e-04 - categorical_accuracy: 1.0000 - val_loss: 5.8881e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 185/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 5.3552e-04 - categorical_accuracy: 1.0000 - val_loss: 5.8912e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 186/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0057 - categorical_accuracy: 1.0000 - val_loss: 5.7322e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 187/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 5.4732e-04 - categorical_accuracy: 1.0000 - val_loss: 5.5403e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 188/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 9.8414e-04 - categorical_accuracy: 1.0000 - val_loss: 5.3699e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 189/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0018 - categorical_accuracy: 1.0000 - val_loss: 5.2973e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 190/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 5.6798e-04 - categorical_accuracy: 1.0000 - val_loss: 5.1872e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 191/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0015 - categorical_accuracy: 1.0000 - val_loss: 5.1188e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 192/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 4.2161e-04 - categorical_accuracy: 1.0000 - val_loss: 5.0222e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 193/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 4.7264e-04 - categorical_accuracy: 1.0000 - val_loss: 4.9278e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 194/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 3.9573e-04 - categorical_accuracy: 1.0000 - val_loss: 4.8526e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 195/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 3.8253e-04 - categorical_accuracy: 1.0000 - val_loss: 4.7870e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 196/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 4.7251e-04 - categorical_accuracy: 1.0000 - val_loss: 4.7083e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 197/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 5.0889e-04 - categorical_accuracy: 1.0000 - val_loss: 4.6291e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 198/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0023 - categorical_accuracy: 1.0000 - val_loss: 4.5678e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 199/300\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 3.8563e-04 - categorical_accuracy: 1.0000 - val_loss: 4.4312e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 200/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0015 - categorical_accuracy: 1.0000 - val_loss: 4.2777e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 201/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 4.0381e-04 - categorical_accuracy: 1.0000 - val_loss: 4.1936e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 202/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0014 - categorical_accuracy: 1.0000 - val_loss: 4.0907e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 203/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0013 - categorical_accuracy: 1.0000 - val_loss: 4.0081e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 204/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 3.6244e-04 - categorical_accuracy: 1.0000 - val_loss: 3.9171e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 205/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 3.3076e-04 - categorical_accuracy: 1.0000 - val_loss: 3.8351e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 206/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 3.5181e-04 - categorical_accuracy: 1.0000 - val_loss: 3.7855e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 207/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 3.3205e-04 - categorical_accuracy: 1.0000 - val_loss: 3.7231e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 208/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 8.3743e-04 - categorical_accuracy: 1.0000 - val_loss: 3.6729e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 209/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 6.6841e-04 - categorical_accuracy: 1.0000 - val_loss: 3.5871e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 210/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 5.1328e-04 - categorical_accuracy: 1.0000 - val_loss: 3.5203e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 211/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 4.0126e-04 - categorical_accuracy: 1.0000 - val_loss: 3.4751e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 212/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 4.0213e-04 - categorical_accuracy: 1.0000 - val_loss: 3.4461e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 213/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 8.4620e-04 - categorical_accuracy: 1.0000 - val_loss: 3.3960e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 214/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 8.5918e-04 - categorical_accuracy: 1.0000 - val_loss: 3.3211e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 215/300\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 3.4034e-04 - categorical_accuracy: 1.0000 - val_loss: 3.2867e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 216/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 3.8127e-04 - categorical_accuracy: 1.0000 - val_loss: 3.2658e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 217/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 8.0106e-04 - categorical_accuracy: 1.0000 - val_loss: 3.1986e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 218/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 4.5893e-04 - categorical_accuracy: 1.0000 - val_loss: 3.1628e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 219/300\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 6.4074e-04 - categorical_accuracy: 1.0000 - val_loss: 3.1457e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 220/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 3.7237e-04 - categorical_accuracy: 1.0000 - val_loss: 3.1271e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 221/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 6.5140e-04 - categorical_accuracy: 1.0000 - val_loss: 3.1015e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 222/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 2.6952e-04 - categorical_accuracy: 1.0000 - val_loss: 3.0583e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 223/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 3.5178e-04 - categorical_accuracy: 1.0000 - val_loss: 3.0447e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 224/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 2.9654e-04 - categorical_accuracy: 1.0000 - val_loss: 3.0217e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 225/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 3.4342e-04 - categorical_accuracy: 1.0000 - val_loss: 2.9852e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 226/300\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 3.7218e-04 - categorical_accuracy: 1.0000 - val_loss: 2.9720e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 227/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 3.2809e-04 - categorical_accuracy: 1.0000 - val_loss: 2.9593e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 228/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0018 - categorical_accuracy: 1.0000 - val_loss: 2.9454e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 229/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 4.8504e-04 - categorical_accuracy: 1.0000 - val_loss: 2.9395e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 230/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 3.0728e-04 - categorical_accuracy: 1.0000 - val_loss: 2.9497e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 231/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 3.9595e-04 - categorical_accuracy: 1.0000 - val_loss: 2.9462e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 232/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0014 - categorical_accuracy: 1.0000 - val_loss: 2.9126e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 233/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 4.9064e-04 - categorical_accuracy: 1.0000 - val_loss: 2.9166e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 234/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0010 - categorical_accuracy: 1.0000 - val_loss: 2.9067e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 235/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 3.8306e-04 - categorical_accuracy: 1.0000 - val_loss: 2.8951e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 236/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 9.9706e-04 - categorical_accuracy: 1.0000 - val_loss: 2.8960e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 237/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0020 - categorical_accuracy: 1.0000 - val_loss: 2.8873e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 238/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 3.5039e-04 - categorical_accuracy: 1.0000 - val_loss: 2.8592e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 239/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0027 - categorical_accuracy: 1.0000 - val_loss: 2.7979e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 240/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 9.2434e-04 - categorical_accuracy: 1.0000 - val_loss: 2.7379e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 241/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 3.1477e-04 - categorical_accuracy: 1.0000 - val_loss: 2.6741e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 242/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0011 - categorical_accuracy: 1.0000 - val_loss: 2.6469e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 243/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 3.5856e-04 - categorical_accuracy: 1.0000 - val_loss: 2.6274e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 244/300\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 7.8620e-04 - categorical_accuracy: 1.0000 - val_loss: 2.6250e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 245/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2.9996e-04 - categorical_accuracy: 1.0000 - val_loss: 2.6006e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 246/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 42ms/step - loss: 2.8544e-04 - categorical_accuracy: 1.0000 - val_loss: 2.5794e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 247/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 3.0071e-04 - categorical_accuracy: 1.0000 - val_loss: 2.5515e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 248/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 2.6826e-04 - categorical_accuracy: 1.0000 - val_loss: 2.5313e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 249/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 3.1784e-04 - categorical_accuracy: 1.0000 - val_loss: 2.5078e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 250/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 3.3592e-04 - categorical_accuracy: 1.0000 - val_loss: 2.5000e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 251/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 3.8737e-04 - categorical_accuracy: 1.0000 - val_loss: 2.4996e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 252/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 4.2001e-04 - categorical_accuracy: 1.0000 - val_loss: 2.5107e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 253/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 3.6876e-04 - categorical_accuracy: 1.0000 - val_loss: 2.5003e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 254/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.9074e-04 - categorical_accuracy: 1.0000 - val_loss: 2.4860e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 255/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 3.6048e-04 - categorical_accuracy: 1.0000 - val_loss: 2.4737e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 256/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 4.7180e-04 - categorical_accuracy: 1.0000 - val_loss: 2.4709e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 257/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 5.6498e-04 - categorical_accuracy: 1.0000 - val_loss: 2.4605e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 258/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 6.4603e-04 - categorical_accuracy: 1.0000 - val_loss: 2.4647e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 259/300\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 6.9131e-04 - categorical_accuracy: 1.0000 - val_loss: 2.4470e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 260/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.6585e-04 - categorical_accuracy: 1.0000 - val_loss: 2.4321e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 261/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 5.3195e-04 - categorical_accuracy: 1.0000 - val_loss: 2.4313e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 262/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.7320e-04 - categorical_accuracy: 1.0000 - val_loss: 2.4134e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 263/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2.6615e-04 - categorical_accuracy: 1.0000 - val_loss: 2.4160e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 264/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 2.5161e-04 - categorical_accuracy: 1.0000 - val_loss: 2.4059e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 265/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 3.7003e-04 - categorical_accuracy: 1.0000 - val_loss: 2.3993e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 266/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 5.2492e-04 - categorical_accuracy: 1.0000 - val_loss: 2.3933e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 267/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 3.1096e-04 - categorical_accuracy: 1.0000 - val_loss: 2.3773e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 268/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 7.3540e-04 - categorical_accuracy: 1.0000 - val_loss: 2.3871e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 269/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.5114e-04 - categorical_accuracy: 1.0000 - val_loss: 2.3892e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 270/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 3.0150e-04 - categorical_accuracy: 1.0000 - val_loss: 2.3715e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 271/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.7686e-04 - categorical_accuracy: 1.0000 - val_loss: 2.3425e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 272/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 5.6556e-04 - categorical_accuracy: 1.0000 - val_loss: 2.3405e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 273/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 2.3792e-04 - categorical_accuracy: 1.0000 - val_loss: 2.3323e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 274/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 3.4440e-04 - categorical_accuracy: 1.0000 - val_loss: 2.3069e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 275/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 3.4025e-04 - categorical_accuracy: 1.0000 - val_loss: 2.3114e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 276/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0017 - categorical_accuracy: 1.0000 - val_loss: 2.2782e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 277/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 3.3508e-04 - categorical_accuracy: 1.0000 - val_loss: 2.2675e-04 - val_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n",
      "Epoch 278/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0035 - categorical_accuracy: 1.0000 - val_loss: 2.2882e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 279/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 5.8153e-04 - categorical_accuracy: 1.0000 - val_loss: 2.3057e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 280/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 2.6858e-04 - categorical_accuracy: 1.0000 - val_loss: 2.3288e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 281/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0037 - categorical_accuracy: 1.0000 - val_loss: 2.4068e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 282/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.4121e-04 - categorical_accuracy: 1.0000 - val_loss: 2.4927e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 283/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0043 - categorical_accuracy: 1.0000 - val_loss: 2.5063e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 284/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 2.4823e-04 - categorical_accuracy: 1.0000 - val_loss: 2.5229e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 285/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 2.4615e-04 - categorical_accuracy: 1.0000 - val_loss: 2.5404e-04 - val_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 286/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.9309e-04 - categorical_accuracy: 1.0000 - val_loss: 2.5437e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 287/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 6.7598e-04 - categorical_accuracy: 1.0000 - val_loss: 2.5319e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 288/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 3.5219e-04 - categorical_accuracy: 1.0000 - val_loss: 2.5225e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 289/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 2.2809e-04 - categorical_accuracy: 1.0000 - val_loss: 2.5146e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 290/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.2767e-04 - categorical_accuracy: 1.0000 - val_loss: 2.5155e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 291/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 2.2830e-04 - categorical_accuracy: 1.0000 - val_loss: 2.5069e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 292/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 6.2457e-04 - categorical_accuracy: 1.0000 - val_loss: 2.4726e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 293/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 1.9757e-04 - categorical_accuracy: 1.0000 - val_loss: 2.4858e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 294/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 2.4524e-04 - categorical_accuracy: 1.0000 - val_loss: 2.4722e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 295/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 2.1850e-04 - categorical_accuracy: 1.0000 - val_loss: 2.4613e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 296/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 3.6366e-04 - categorical_accuracy: 1.0000 - val_loss: 2.4553e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 297/300\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 2.8670e-04 - categorical_accuracy: 1.0000 - val_loss: 2.4686e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 298/300\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.7637e-04 - categorical_accuracy: 1.0000 - val_loss: 2.4456e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 299/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 3.2749e-04 - categorical_accuracy: 1.0000 - val_loss: 2.4382e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 300/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 3.9077e-04 - categorical_accuracy: 1.0000 - val_loss: 2.4462e-04 - val_categorical_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "batch_size = 64\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        \"cnn_model\", save_best_only=True, monitor=\"val_loss\"\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=7, min_lr=0.0001\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1),\n",
    "]\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"categorical_accuracy\"],\n",
    ")\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_split=0.2,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9710532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step - loss: 6.0452e-04 - categorical_accuracy: 1.0000\n",
      "Test accuracy 1.0\n",
      "Test loss 0.000604515487793833\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"Test accuracy\", test_acc)\n",
    "print(\"Test loss\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cfee131e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 201ms/step - loss: 6.9370e-04 - categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0006936962599866092, 1.0]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#modelx = tf.keras.models.load_model(\"./cnn_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7323f62d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'categorical_accuracy', 'val_loss', 'val_categorical_accuracy', 'lr'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fec84e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = \"categorical_accuracy\"\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(history.history[metric])\n",
    "plt.plot(history.history[\"val_\" + metric])\n",
    "plt.title(\"model \" + metric)\n",
    "plt.ylabel(metric, fontsize=\"large\")\n",
    "plt.xlabel(\"epoch\", fontsize=\"large\")\n",
    "plt.legend([\"train\", \"val\"], loc=\"best\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3758014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ScopedTFGraph.__del__ at 0x00000182AA7E5948>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\mudaafi\\desktop\\nus\\cs3237 - iot\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\c_api_util.py\", line 58, in __del__\n",
      "    self.deleter(self.graph)\n",
      "AttributeError: deleter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: Push ups vs Actual: Push ups\n",
      "Predicted: Running vs Actual: Running\n",
      "Predicted: Skipping vs Actual: Skipping\n",
      "Predicted: Push ups vs Actual: Push ups\n",
      "Predicted: Skipping vs Actual: Skipping\n",
      "Predicted: Push ups vs Actual: Push ups\n",
      "Predicted: Push ups vs Actual: Push ups\n",
      "Predicted: Skipping vs Actual: Skipping\n",
      "Predicted: Running vs Actual: Running\n",
      "Predicted: Running vs Actual: Running\n",
      "Predicted: Skipping vs Actual: Skipping\n",
      "Predicted: Push ups vs Actual: Push ups\n",
      "Predicted: Running vs Actual: Running\n",
      "Predicted: Push ups vs Actual: Push ups\n",
      "Predicted: Push ups vs Actual: Push ups\n",
      "Predicted: Running vs Actual: Running\n",
      "Predicted: Push ups vs Actual: Push ups\n",
      "\n",
      "              ======================\n",
      "Accuracy for Running: {.2f}% 100.0\n",
      "Accuracy for Push Ups: {.2f}% 100.0\n",
      "Accuracy for Skipping: {.2f}% 100.0\n"
     ]
    }
   ],
   "source": [
    "evaluate_categorical_predictions(model, x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
